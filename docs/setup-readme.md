MLOps GPU Platform â€” Full Recap (What We Built + Why It Matters)

You now have a real-world, production-style MLOps platform with GPU inference, CI/CD, observability, and infrastructure automation â€” but built in a lean, fast, teardown-friendly way.

Below is the complete breakdown.

ðŸ”¥ 1. High-Level Architecture
GitHub â†’ CI/CD â†’ ECR â†’ EKS â†’ GPU Node â†’ Triton Server â†’ Prometheus â†’ Grafana

Each component plays a specific role:
Layer	Purpose
GitHub Repo	Holds code, models, Dockerfile, Helm charts, Terraform
GitHub Actions CI/CD	Builds image â†’ pushes to ECR â†’ deploys to EKS
Terraform Infrastructure	Provisions VPC, EKS, GPU NodeGroup, IAM, OIDC
Triton Inference Server (Docker)	Runs all ML models on GPU
Helm Charts	Deploy Triton + Prometheus + Grafana
ECR	Stores built GPU inference container images
Prometheus	Scrapes GPU + Triton metrics
Grafana	Dashboards: GPU usage, Triton latency, throughput
ðŸ§© 2. Repository Folder Structure (Final)

You organized your repo like a real company-style MLOps mono-repo.

mlops-gpu-platform/
â”œâ”€â”€ ci/
â”‚   â””â”€â”€ github-actions/
â”‚       â””â”€â”€ build-push-triton.yaml      # CI/CD pipeline
â”‚
â”œâ”€â”€ docs/                                # Architecture documentation
â”‚   â”œâ”€â”€ iam-roles.md
â”‚   â”œâ”€â”€ infra-readme.md
â”‚   â””â”€â”€ vpc-setup.md
â”‚
â”œâ”€â”€ infra/                               # ALL infra as code
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ main.tf                      # Core EKS stack
â”‚       â”œâ”€â”€ provider.tf
â”‚       â”œâ”€â”€ variables.tf
â”‚       â”œâ”€â”€ outputs.tf
â”‚       â”œâ”€â”€ aws-auth.yaml
â”‚       â”œâ”€â”€ terraform.tfvars
â”‚       â”œâ”€â”€ modules/
â”‚       â”‚   â”œâ”€â”€ vpc/                     # Subnets, routing, gateways
â”‚       â”‚   â”œâ”€â”€ eks/                     # EKS cluster creation
â”‚       â”‚   â”œâ”€â”€ gpu_node_group/          # GPU nodes w/ taints
â”‚       â”‚   â”œâ”€â”€ nvidia_plugin/           # Device plugin
â”‚       â”‚   â””â”€â”€ iam/                      # Roles including GitHub OIDC
â”‚       â”‚       â”œâ”€â”€ github_actions_role.tf
â”‚       â”‚       â””â”€â”€ templates/ci.json.tpl
â”‚       â”œâ”€â”€ policies/
â”‚       â”‚   â”œâ”€â”€ alb.json
â”‚       â”‚   â”œâ”€â”€ ci.json (replaced by template)
â”‚       â”‚   â””â”€â”€ eks.json
â”‚       â””â”€â”€ logs/
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ triton/
â”‚       â”œâ”€â”€ Dockerfile                    # Triton+Models container
â”‚       â”œâ”€â”€ export_resnet50.py            # Model export script
â”‚       â”œâ”€â”€ validate-model.py             # Sanity check
â”‚       â”œâ”€â”€ models/
â”‚       â”‚   â”œâ”€â”€ resnet50/                 # ONNX + config.pbtxt
â”‚       â”‚   â””â”€â”€ gpt_mini/                 # Small LLM model
â”‚       â””â”€â”€ helm/                         # Helm chart for Triton
â”‚           â””â”€â”€ templates/
â”‚               â”œâ”€â”€ deployment.yaml
â”‚               â”œâ”€â”€ service.yaml
â”‚               â”œâ”€â”€ hpa.yaml
â”‚               â”œâ”€â”€ values.yaml
â”‚
â””â”€â”€ monitoring/
    â”œâ”€â”€ prometheus-helm/
    â”œâ”€â”€ grafana-helm/
    â””â”€â”€ dashboards/
        â”œâ”€â”€ gpu.json
        â”œâ”€â”€ triton-metrics.json
        â”œâ”€â”€ latency-dashboard.json

ðŸŽ¯ 3. What Components We Built
A. Triton Server Model Deployment

Export ResNet50 â†’ ONNX

Create config.pbtxt

Build a container with models baked in

Run Triton locally (GPU-node validated)

Created Helm chart to deploy Triton in Kubernetes

This simulates how companies deploy inference services in production.

B. Terraform Infrastructure as Code

We built a reusable stack:

Includes:

VPC

Subnets

Internet Gateway / NAT

EKS Cluster

GPU Node Group

IAM roles

EKS

NodeRole

Operator roles

GitHub OIDC â†’ AWS (for CI/CD)

GPU-specific features:

Node taints: gpu=true:NoSchedule

NVIDIA device plugin Helm release

ECR access for pulling Triton images

C. CI/CD Pipeline

Inside:

ci/github-actions/build-push-triton.yaml


Pipeline does:

Authenticate to AWS using OIDC (no secrets)

Build Docker image

Tag + push to ECR

Deploy to EKS using Helm upgrade

This is full enterprise CI/CD.

D. Observability

You added full GPU inference observability:

1. Prometheus Stack (Helm)

Scrapes:

Node GPU metrics (DCGM)

Triton server metrics

Cluster metrics (cadvisor, kube-state-metrics)

2. Grafana (Helm)

Dashboards included:

GPU Utilization

GPU Memory

Triton model load time

Model throughput (infer/sec)

Latency P50/P90/P99

3. Slack Alerts

GPU > 90%

Triton error rate > 1%

Pod Restarts > 2

This is production-grade quality.

ðŸš€ 4. How All Components Tie Together (End-to-End Flow)
â‡¢ Developer Workflow

You push code or new model to GitHub

GitHub Actions builds + pushes Triton container to ECR

GitHub Actions deploys new version to EKS using Helm

EKS schedules Triton pods on GPU node

Triton loads your model repository

Metrics flow to Prometheus

Dashboards visualize usage in Grafana

ðŸ’¸ 5. Project is Production-Style But Cost-Minimized

You designed this to be:

Zero idle cost (GPU node scales to 0)

Easily destroyed with:

terraform destroy


No secrets stored

No paid add-ons

Free + open-source monitoring

Smallest GPU node (g4dn.xlarge)

Shutdown GPU overnight

For learning, this is extremely efficient.

ðŸ§  6. What You Learned (Skills Gained)

You now understand:

ML Engineering

ONNX model export / packaging

Configuring Triton model repositories

Understanding batching / dynamic batching

MLOps

CI/CD with GitHub OIDC

Multi-stage Docker builds

Model versioning

Helm charts + deployments

Infrastructure

Terraform modules

EKS cluster creation

GPU node groups

Device plugin installation

Observability

Prometheus exporters

GPU dashboards

Triton metrics

Alerting paths

This is a real MLOps engineer/ML Infra engineer skillset.

ðŸ”¥ 7. Whatâ€™s Next / Optional Enhancements


1. Add autoscaling based on Triton throughput
2. Add distributed inference using multiple GPUs
3. Add Redis/MLflow for model registry
4. Add API gateway + auth
5. Add multi-model ensemble pipelines in Triton


Generate: 
âœ… End-to-end architecture diagram
âœ… Terraform graph visualization
âœ… Triton request flow diagrams
âœ… Prometheus + Grafana architecture diagram
